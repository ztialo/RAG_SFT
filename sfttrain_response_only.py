# -*- coding: utf-8 -*-
"""SFTtrain response only.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-pk0RZ0PxtvvI_rLsOVlFFgU2Ydr2M44
"""

# Commented out IPython magic to ensure Python compatibility.

"""
# Install required versions
# Install Unsloth stable version
pip uninstall -y transformers peft unsloth unsloth_zoo datasets trl xformers accelerate bitsandbytes
pip install transformers==4.46.3 peft==0.12.0 
pip install --upgrade --force-reinstall datasets==2.19.1
pip install unsloth

"""
import os
os.listdir()

import unsloth
from unsloth import FastLanguageModel
import torch

max_seq_length = 1536 # TinyLlama handles 2048 natively. Unsloth for tinyllama: https://huggingface.co/unsloth/tinyllama
dtype = None
load_in_4bit = True

model, tokenizer = FastLanguageModel.from_pretrained(
    # model_name = "unsloth/tinyllama-bnb-4bit",
    model_name = "unsloth/Llama-3.2-1B-Instruct-bnb-4bit",
    max_seq_length = max_seq_length,
    dtype = dtype,
    load_in_4bit = load_in_4bit,
)

# Add LoRA adapters
model = FastLanguageModel.get_peft_model(
    model,
    r = 32, # TinyLlama benefits from higher rank
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj",
                    #   "embed_tokens", "lm_head",
                      ],
    lora_alpha = 32,
    lora_dropout = 0,
    bias = "none",
    use_gradient_checkpointing = False,
    random_state = 3407,
    use_rslora = False,
    loftq_config = None,
)

from datasets import load_dataset

alpaca_prompt = """Below is an prompt that includes question and context. Write a response that appropriately completes the request.

### Prompt:
{}

### Response:
{}"""

# 7b response text: prompt(input context) +### Response<response> -> tokens abc
# sft: 1B asbdiawhdiaubwd
# 7b response - 1b response = loss of response

EOS_TOKEN = tokenizer.eos_token

def formatting_prompts_func(examples):
    texts = []
    for inp, ctx, resp in zip(examples["input"], examples["context"], examples["response"]):

        # combine input + context
        prompt = inp
        if ctx and len(str(ctx).strip()) > 0:
            prompt = f"{inp}\n{ctx}"

        text = alpaca_prompt.format(prompt, resp) + EOS_TOKEN
        texts.append(text)

    # for dataset.map we return a dict
    return {"text": texts}

# Load thy dataset
dataset = load_dataset("json", data_files="rolling_rag_gemma_qwen7b_q555.json", split="train")
dataset = dataset.map(formatting_prompts_func, batched=True, load_from_cache_file=False)

# print(dataset[0]["text"])
# print("### Prompt:" in dataset[0]["text"])
# print("### Response:" in dataset[0]["text"])

def tokenize_function(examples):
    return tokenizer(
        examples["text"],
        truncation=True,
        max_length=max_seq_length,
        padding="max_length",
        return_attention_mask=True,
    )

dataset = dataset.map(
    tokenize_function,
    batched=True,
    remove_columns=dataset.column_names,  # keep only model-ready fields
)

from dataclasses import dataclass
import torch

@dataclass
class CompletionOnlyCollator:
    tokenizer: object
    response_template: str = "\n### Response:"

    def __post_init__(self):
        # tokenize the delimiter once
        self.response_template = "\n### Response:"

    def __call__(self, batch):
        input_ids_list = []
        attention_masks = []
        labels_list = []

        for ex_idx, ex in enumerate(batch):
            ids = ex["input_ids"]

            # ✅ safely get attention_mask or create a default one
            mask = ex.get("attention_mask", None)
            if mask is None:
                # no mask in dataset → assume all tokens are non-padding
                mask = [1] * len(ids)

            if isinstance(ids, torch.Tensor):
                ids = ids.tolist()
            if isinstance(mask, torch.Tensor):
                mask = mask.tolist()

            labels = ids.copy()

            # 1) find "### Response:" by decoding and searching for the string
            full_text = self.tokenizer.decode(ids)
            response_start_idx = full_text.find(self.response_template)
            
            if response_start_idx == -1:
                # no marker → mask everything
                labels = [-100] * len(labels)
                if ex_idx == 0:
                    print(f"[DEBUG] No response marker found in text!")
                    # print(f"[DEBUG] Text preview: {full_text[:200]}...")
            else:
                # Find the token position where response starts
                # Encode text up to response marker, get token count
                text_before_response = full_text[:response_start_idx + len(self.response_template)]
                tokens_before = len(self.tokenizer.encode(text_before_response, add_special_tokens=False))
                
                if ex_idx == 0:
                    print(f"[DEBUG] Response marker found in text at char position: {response_start_idx}")
                    # print(f"[DEBUG] Token position after marker: {tokens_before}")
                
                # 2) mask everything before the response
                for j in range(tokens_before):
                    labels[j] = -100

            # 3) also mask padding tokens (attention_mask == 0)
            for j, m in enumerate(mask):
                if m == 0:
                    labels[j] = -100

            # DEBUG: Show label comparison
            masked_count = sum(1 for l in labels if l == -100)
            total_tokens = len(labels)
            # print(f"  Masked tokens: {masked_count}/{total_tokens} ({100*masked_count/total_tokens:.1f}%)")

            input_ids_list.append(torch.tensor(ids))
            attention_masks.append(torch.tensor(mask))
            labels_list.append(torch.tensor(labels))

        return {
            "input_ids": torch.stack(input_ids_list),
            "attention_mask": torch.stack(attention_masks),
            "labels": torch.stack(labels_list),
        }

from trl import SFTTrainer
from transformers import TrainingArguments
from unsloth.chat_templates import train_on_responses_only

data_collator = CompletionOnlyCollator(
    tokenizer = tokenizer,
    response_template = "\n### Response:",  # matches your template
)

trainer = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    train_dataset = dataset,       # already tokenized
    data_collator = data_collator, # <-- response-only loss logic
    max_seq_length = max_seq_length,
    dataset_num_proc = 2,
    packing = False,
    args = TrainingArguments(
        per_device_train_batch_size = 8,
        gradient_accumulation_steps = 2,
        warmup_steps = 10,
        max_steps = 120,
        learning_rate = 2e-4,
        fp16 = not torch.cuda.is_bf16_supported(),
        bf16 = torch.cuda.is_bf16_supported(),
        logging_steps = 1,
        optim = "adamw_8bit",
        weight_decay = 0.01,
        lr_scheduler_type = "linear",
        seed = 3407,
        output_dir = "outputs",
    ),
)
# batch = [dataset[0]]
# masked = data_collator(batch)

# print("\n" + "="*80)
# print("FULL INPUT:\n", tokenizer.decode(masked["input_ids"][0]))

# space_id = tokenizer(" ", add_special_tokens=False).input_ids[0]
# decoded_labels = tokenizer.decode([
#     tok if tok != -100 else space_id
#     for tok in masked["labels"][0].tolist()
# ])
# print("\nLABEL TEXT (should be just the response):\n", decoded_labels)

# # Additional verification: show which tokens are being trained on
# labels_tensor = masked["labels"][0].tolist()
# trained_token_ids = [tok for tok in labels_tensor if tok != -100]
# print(f"\nTokens being trained on: {len(trained_token_ids)}/{len(labels_tensor)}")
# print(f"Sample trained token IDs: {trained_token_ids[:20]}")
# print("="*80 + "\n")
trainer.train()

# Save to GGUF
model.save_pretrained_merged(
    "llama3.2-rag-v3--hf",  # output HF folder name
    tokenizer,
    save_method="merged_16bit",
)

# Convert hf to gguf
"""
conda activate llama
# change the version number
python llama.cpp/convert_hf_to_gguf.py \
    llama3.2-rag-v2-hf \
    --outfile Models/llama3.2-rag-v2.gguf \
    --outtype q8_0
-> create modefile
ollama create llama3.2_trained_v2 -f Modelfile
"""